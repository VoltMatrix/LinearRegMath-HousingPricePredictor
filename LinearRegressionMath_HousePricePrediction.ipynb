{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "odMUVCLmP-rH"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datafile= 'data/ex1data1.txt'\n",
        "cols= np.loadtxt(datafile, delimiter=',',usecols=(0,1),unpack=True)"
      ],
      "metadata": {
        "id": "tILQ9cw_QCGt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Reads a text file (ex1data1.txt) with two columns (e.g., city population and restaurant profit) into cols. The delimiter=',' means the file is comma-separated, and usecols=(0,1) selects the first two columns. unpack=True splits the data into separate arrays.\n",
        "Why: The dataset contains our input (e.g., population) and output (e.g., profit) for training the model. We need to load it into a format we can work with."
      ],
      "metadata": {
        "id": "tyUKjlgpYMD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X= np.transpose(np.array(cols[:-1]))\n",
        "y= np.transpose(np.array(cols[-1:]))\n",
        "m=y.size"
      ],
      "metadata": {
        "id": "AMy2Uy1TQCJS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Adds a column of 1s to the left of X (so X now has two columns: one of 1s, one for the feature).\n",
        "Why: This is for the bias term (intercept) in linear regression. The model is $ h(x) = \\theta_0 + \\theta_1 x $, where $\\theta_0$ is the intercept. The column of 1s lets us include $\\theta_0$ in the matrix math (more on this later)."
      ],
      "metadata": {
        "id": "MmF2aONHYowG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X= np.insert(X,0,1,axis=1)"
      ],
      "metadata": {
        "id": "LZlU4_dNQCMN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(X[:,1],y[:,0],'rx',markersize=10)\n",
        "plt.grid(True)\n",
        "plt.ylabel(\"profit in $10000s\")\n",
        "plt.xlable(\"Population of City in 10,000s\")"
      ],
      "metadata": {
        "id": "7zkrUopQQCPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Gradient Descent for Single-Variable Linear Regression\n",
        "\"\"\"Linear regression predicts $ y = \\theta_0 + \\theta_1 x $ by finding the best $\\theta_0$ (intercept) and $\\theta_1$ (slope). Gradient descent iteratively adjusts $\\theta_0$ and $\\theta_1$ to minimize\n",
        " the cost function, which measures how far off predictions are\n",
        " from actual values\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "4xtwf3-QQCSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Sets iterations (how many times to update $\\theta$) to 1500 and alpha (learning rate, how big each update step is) to 0.01.\n",
        "Why: Gradient descent needs these parameters. iterations controls how long we optimize, and alpha controls how fast we adjust $\\theta$. Too large an alpha can overshoot; too small makes it slow."
      ],
      "metadata": {
        "id": "h2oNwfbOZ_sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iterations= 1500\n",
        "alpha=0.01"
      ],
      "metadata": {
        "id": "I8HM_eDwQCU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Defines the hypothesis function $ h(\\theta, X) = X \\theta $, which computes predictions. X is the input matrix (with 1s and features), theta is the parameter vector ($\\theta_0, \\theta_1$), and np.dot does matrix multiplication.\n",
        "Why: This is the linear regression model. For each input row in X, it computes $ \\theta_0 \\cdot 1 + \\theta_1 \\cdot x $, giving the predicted profit. In sklearn, this is what model.predict(X) does internally."
      ],
      "metadata": {
        "id": "JIyzhvi4abcd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def h(theta,X):\n",
        "  return np.dot(X, theta)"
      ],
      "metadata": {
        "id": "I0o3LEQwQCXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Calculates the cost function $ J(\\theta) = \\frac{1}{2m} \\sum_{i=1}^m (h(x_i) - y_i)^2 $, where h(mytheta, X) is the prediction, h(mytheta, X) - y is the error, and the rest computes the mean squared error.\n",
        "Why: The cost function measures how bad the predictions are. Gradient descent minimizes this. In sklearn, this is what the model minimizes when you call fit()."
      ],
      "metadata": {
        "id": "Iq0ljpRYcBSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeCost(mytheta, X, y):\n",
        "  return float(  (1./(2*m))* np.dot((h(mytheta, X)-y).T,(h(mytheta,X) -y)))"
      ],
      "metadata": {
        "id": "XU9jkwxVQCam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Initializes $\\theta$ as a zero vector (shape matches X’s columns: 2x1 for $\\theta_0, \\theta_1$) and computes the cost with $\\theta = [0, 0]$, printing 32.07.\n",
        "Why: Starting with zeros is a common initial guess. The cost (32.07) shows how bad the model is before optimization. This is like checking the initial error in sklearn."
      ],
      "metadata": {
        "id": "gIEV_Dzjcsx7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_theta= np.zeros((X.shape[1],1))\n",
        "print(computeCost(initial_theta,X,y))"
      ],
      "metadata": {
        "id": "Q7lWFqnDQCde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Implements gradient descent. It:\n",
        "\n",
        "Starts with theta_start (zeros).\n",
        "Tracks the cost (jvec) and $\\theta$ values (thetahistory) at each iteration.\n",
        "Updates each $\\theta_j$ using the gradient descent rule: $ \\theta_j = \\theta_j - \\frac{\\alpha}{m} \\sum_{i=1}^m (h(x_i) - y_i) x_{ij} $.\n",
        "Returns final $\\theta$, history of $\\theta$, and cost history.\n",
        "\n",
        "\n",
        "Why: This is the core of linear regression without sklearn. It iteratively adjusts $\\theta$ to reduce the cost function, like how sklearn’s LinearRegression.fit() optimizes parameters. thetahistory and jvec help visualize the process."
      ],
      "metadata": {
        "id": "wvqjCezffNRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def descendGradient(X, theta_start=np.zeros(2)):\n",
        "  theta= theta_start\n",
        "  jvec= []\n",
        "  thetahistory=[]\n",
        "  for _ in range(iterations):\n",
        "    tmptheta=theta\n",
        "    jvec.append(computeCost(theta, X, y))\n",
        "    thetahistory.append(list(theta[:,0]))\n",
        "    for j in range(len(tmptheta)):\n",
        "      theta[j]= theta[j] - (alpha/m) * np.sum( (h(theta,X)-y) * np.array(X[:,j]).reshape(m,1))\n",
        "    theta= tmptheta\n",
        "  return theta, thetahistory, jvec"
      ],
      "metadata": {
        "id": "Kbyuo7IHctat"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Runs gradient descent with initial $\\theta = [0, 0]$, getting the optimized $\\theta$, history of $\\theta$, and cost values.\n",
        "Why: This trains the model, finding the best $\\theta_0, \\theta_1$ for the line $ y = \\theta_0 + \\theta_1 x $. It’s like calling model.fit(X, y) in sklearn."
      ],
      "metadata": {
        "id": "OT6TOHYnfiUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_theta= np.zeros((X.shape[1], 1))\n",
        "theta, thetahhistory, jvec= descendGradient(X,initial_theta)"
      ],
      "metadata": {
        "id": "19Cea5_mQCgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Plots the cost (jvec) over iterations as blue dots.\n",
        "Why: Shows how the cost decreases as gradient descent runs, confirming the model is learning. In sklearn, you don’t see this, but it’s what happens internally."
      ],
      "metadata": {
        "id": "V5FQHvj7fpjT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plotConvergence(jvec):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(range(len(jvec)), jvec, 'bo')\n",
        "    plt.grid(True)\n",
        "    plt.title(\"Convergence of Cost Function\")\n",
        "    plt.xlabel(\"Iteration number\")\n",
        "    plt.ylabel(\"Cost function\")\n",
        "    plt.xlim([-0.05*iterations, 1.05*iterations])\n",
        "    plt.ylim([4, 7])"
      ],
      "metadata": {
        "id": "yJklaSZBQCjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Defines myfit to compute predictions using the learned $\\theta$. Plots the data (red 'x') and the fitted line (blue) with the equation $ h(x) = \\theta_0 + \\theta_1 x $.\n",
        "Why: Visualizes how well the model fits the data, like plotting model.predict(X) in sklearn. The legend shows the learned equation.\n",
        "\n"
      ],
      "metadata": {
        "id": "_0UPEHpefxwa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def myfit(xval):\n",
        "    return theta[0] + theta[1]*xval\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(X[:,1], y[:,0], 'rx', markersize=10, label='Training Data')\n",
        "plt.plot(X[:,1], myfit(X[:,1]), 'b-', label='Hypothesis: h(x) = %0.2f + %0.2fx'%(theta[0], theta[1]))\n",
        "plt.grid(True)\n",
        "plt.ylabel('Profit in $10,000s')\n",
        "plt.xlabel('Population of City in 10,000s')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "WM1kMBRLQCmk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Creates a 3D plot of the cost function $ J(\\theta_0, \\theta_1) $ over a grid of $\\theta_0$ and $\\theta_1$ values. Colors show cost magnitude, and a blue line traces the path of $\\theta$ during gradient descent.\n",
        "Why: This visualizes the “bowl-shaped” cost function and how gradient descent moves toward the minimum. It’s a teaching tool to understand optimization, not something sklearn shows but useful for learning."
      ],
      "metadata": {
        "id": "vtTmhbkMf36K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from mpl_toolkits.mplot3d import axes3d, Axes3D\n",
        "from matplotlib import cm\n",
        "import itertools\n",
        "fig = plt.figure(figsize=(12, 12))\n",
        "ax = fig.gca(projection='3d')\n",
        "xvals = np.arange(-10, 10, .5)\n",
        "yvals = np.arange(-1, 4, .1)\n",
        "myxs, myys, myzs = [], [], []\n",
        "for david in xvals:\n",
        "    for kaleko in yvals:\n",
        "        myxs.append(david)\n",
        "        myys.append(kaleko)\n",
        "        myzs.append(computeCost(np.array([[david], [kaleko]]), X, y))\n",
        "scat = ax.scatter(myxs, myys, myzs, c=np.abs(myzs), cmap=plt.get_cmap('YlOrRd'))\n",
        "plt.xlabel(r'$\\theta_0$', fontsize=30)\n",
        "plt.ylabel(r'$\\theta_1$', fontsize=30)\n",
        "plt.title('Cost (Minimization Path Shown in Blue)', fontsize=30)\n",
        "plt.plot([x[0] for x in thetahhistory], [x[1] for x in thetahhistory], jvec, 'bo-')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YWzinq0LfzVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Loads a new dataset with three columns (size, bedrooms, price). Forms X (two features + 1s column) and y (price). m is the number of examples.\n",
        "Why: Same as before, but now we have two features, so $ h(x) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 $. This is like using multiple features in sklearn’s X."
      ],
      "metadata": {
        "id": "F2Un8gj8gNKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "datafile = 'data/ex1data2.txt'\n",
        "cols = np.loadtxt(datafile, delimiter=',', usecols=(0,1,2), unpack=True)\n",
        "X = np.transpose(np.array(cols[:-1]))\n",
        "y = np.transpose(np.array(cols[-1:]))\n",
        "m = y.size\n",
        "X = np.insert(X, 0, 1, axis=1)\n"
      ],
      "metadata": {
        "id": "5JX0fmFkfzYD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Plots histograms of the columns of X (1s, size, bedrooms).\n",
        "Why: Shows the range of values. The 1s are constant, but size (e.g., 1000–4000 sq ft) and bedrooms (e.g., 1–5) have different scales, suggesting we need normalization."
      ],
      "metadata": {
        "id": "Tz-YmvZVgS2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid(True)\n",
        "plt.xlim([-100, 5000])\n",
        "dummy = plt.hist(X[:,0], label='col1')\n",
        "dummy = plt.hist(X[:,1], label='col2')\n",
        "dummy = plt.hist(X[:,2], label='col3')\n",
        "plt.title('Clearly we need feature normalization.')\n",
        "plt.xlabel('Column Value')\n",
        "plt.ylabel('Counts')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "sFJlIXOufzaz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Normalizes features in Xnorm by subtracting the mean and dividing by the standard deviation for each feature (except the 1s column). Stores means and stds for later use.\n",
        "Why: Features like size (large numbers) and bedrooms (small numbers) have different scales, which can make gradient descent unstable. Normalization makes them comparable (like sklearn’s StandardScaler)."
      ],
      "metadata": {
        "id": "iQ73kq8qhWs8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stored_feature_means, stored_feature_stds= [],[]\n",
        "Xnorm= X.copy()\n",
        "for icol in range(Xnorm.shape[1]):\n",
        "  stored_feature_means.append(np.mean(Xnorm[:, icol]))\n",
        "  stored_feature_stds.append(np.mean(Xnorm[:,icol]))\n",
        "  if not icol: continue\n",
        "  Xnorm[:, icol] = (Xnorm[:,icol]- stored_feature_means[-1]) / stored_feature_stds[-1]"
      ],
      "metadata": {
        "id": "VT3Mu6TUfzdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Plots histograms of normalized features.\n",
        "Why: Confirms normalization worked—features now have similar scales (centered around 0, spread ~1), making gradient descent more efficient."
      ],
      "metadata": {
        "id": "iBlJgJWihbDq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid(True)\n",
        "plt.xlim([-5, 5])\n",
        "dummy = plt.hist(Xnorm[:,0], label='col1')\n",
        "dummy = plt.hist(Xnorm[:,1], label='col2')\n",
        "dummy = plt.hist(Xnorm[:,2], label='col3')\n",
        "plt.title('Feature Normalization Accomplished')\n",
        "plt.xlabel('Column Value')\n",
        "plt.ylabel('Counts')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "_dJzpu0FhRh8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Runs gradient descent on normalized Xnorm with initial $\\theta = [0, 0, 0]$ (three parameters for bias, size, bedrooms).\n",
        "Why: Trains the model for multiple features, like model.fit(X, y) in sklearn but with normalized data."
      ],
      "metadata": {
        "id": "pJpEpqHsh2VZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "initial_theta= np.zeros((Xnorm.shape[1], 1))\n",
        "theta, thetahhistory, jvec = descendGradient(Xnorm, initial_theta)"
      ],
      "metadata": {
        "id": "nNGeTaSlhRlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Plots the cost over iterations.\n",
        "Why: Verifies that gradient descent is converging (cost decreases), like checking training progress in sklearn."
      ],
      "metadata": {
        "id": "zwaDESe6h9sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plotConvergence(jvec)"
      ],
      "metadata": {
        "id": "AzB3jHXRhRn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Predicts the price of a house with 1650 sq ft and 3 bedrooms. Normalizes the input (ytest) using stored means and stds, adds a 1 for the bias, and computes the prediction using $ h(\\theta, ytestscaled) $.\n",
        "Why: To make a prediction, we normalize the input to match the training data’s scale, then use the learned $\\theta$. This is like model.predict([[1650, 3]]) in sklearn after scaling."
      ],
      "metadata": {
        "id": "e91V_I6PkG_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Check of result: What is price of house with 1650 square feet and 3 bedrooms?\")\n",
        "ytest= np.array([1650.,3.])\n",
        "ytestscaled= [  (ytest[X] - stored_feature_means[X + 1])/ stored_feature_stds[X + 1] for X in range(len(ytest))]\n",
        "ytestscaled.insert(0,1)\n",
        "print(\"$%0.2f\" % float(h(theta, ytestscaled)))"
      ],
      "metadata": {
        "id": "TRI5w9M6hRrG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What: Defines the normal equation, $ \\theta = (X^T X)^{-1} X^T y $, which analytically solves for $\\theta$ without iteration. Uses it to predict the house price for 1650 sq ft and 3 bedrooms.\n",
        "Why: The normal equation is an alternative to gradient descent that directly computes the optimal $\\theta$. It’s like sklearn’s LinearRegression default method but doesn’t require normalization or iteration. It’s used here to compare results with gradient descent."
      ],
      "metadata": {
        "id": "c0cm5DrTk4Fo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normEqtn(X, y):\n",
        "    return np.dot(np.dot(inv(np.dot(X.T, X)), X.T), y)\n",
        "print(\"Normal equation prediction for price of house with 1650 square feet and 3 bedrooms\")\n",
        "print(\"$%0.2f\" % float(h(normEqtn(X, y), [1, 1650., 3])))"
      ],
      "metadata": {
        "id": "0wi50KKEgT3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NKhRRu2FgT5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CjBa52vcgT_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QGcFO8RPgUCu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}